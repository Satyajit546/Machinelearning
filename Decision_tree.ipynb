{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. What is Decision Tree and how does it work?"
      ],
      "metadata": {
        "id": "xUgC6f9okqnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A decision tree is a supervised machine learning algorithm that can be used for both classification and regression tasks. It's a flowchart-like structure where each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label (in classification) or a numerical value (in regression).\n",
        "\n",
        "How does it work?\n",
        "\n",
        "Data Preparation: The algorithm starts with a labeled dataset, where each instance is described by a set of attributes (features) and a corresponding target variable (class label or numerical value).\n",
        "Tree Construction: The tree is built recursively, starting from the root node.\n",
        "Attribute Selection: At each node, the algorithm selects the best attribute to split the data based on a purity measure (e.g., Gini impurity, information gain). The goal is to find the attribute that best separates the data into subsets with similar target values.\n",
        "Splitting: The data is split into subsets based on the selected attribute's values.\n",
        "Recursion: Steps 3 and 4 are repeated for each subset until a stopping criterion is met (e.g., all instances in a subset belong to the same class, a maximum depth is reached).\n",
        "Prediction: To predict the target value for a new instance, the instance is traversed down the tree based on its attribute values until it reaches a leaf node. The prediction is the class label or numerical value associated with the leaf node.\n",
        "\n",
        "Example:\n",
        "\n",
        "Imagine you want to predict whether a customer will buy a product based on their age and income. A decision tree might look like this:\n",
        "\n",
        "Root Node: Age < 30?\n",
        "Branch 1 (Yes): Income < 50k?\n",
        "Leaf Node 1 (Yes): Will not buy\n",
        "Leaf Node 2 (No): Will buy\n",
        "Branch 2 (No): Will buy\n",
        "In this example, the tree first checks the customer's age. If they are younger than 30, it further checks their income. If they are older than 30, it predicts they will buy the product regardless of their income.\n",
        "\n",
        "#Advantages of Decision Trees:\n",
        "\n",
        "Easy to understand and interpret.\n",
        "Can handle both categorical and numerical data.\n",
        "Non-parametric, meaning they don't make assumptions about the data distribution.\n",
        "Can be used for feature selection.\n",
        "#Disadvantages of Decision Trees:\n",
        "\n",
        "Prone to overfitting if not pruned properly.\n",
        "Can be unstable, meaning small changes in the data can lead to large changes in the tree structure.\n",
        "May not perform well on complex datasets with many features and interactions."
      ],
      "metadata": {
        "id": "Yr6xhlb2lCX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.What are impurity measures in Decision Tree?"
      ],
      "metadata": {
        "id": "YPiKR6K8lWC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Impurity Measures\n",
        "\n",
        "Impurity measures are used by decision tree algorithms to determine the best way to split the data at each node of the tree. The goal is to find the attribute that best separates the data into subsets with similar target values. This is done by selecting the attribute that minimizes the impurity of the resulting subsets.\n",
        "\n",
        "There are several common impurity measures used in decision trees, including:\n",
        "\n",
        "#Gini Impurity:\n",
        "\n",
        "Gini impurity is a measure of the probability of misclassifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.\n",
        "It ranges from 0 (pure) to 1 (impure).\n",
        "A lower Gini impurity indicates a better split.\n",
        "Formula: Gini Impurity = 1 - Σ (pi)^2, where pi is the proportion of instances belonging to class i in a node.\n",
        "Entropy:\n",
        "\n",
        "Entropy is a measure of the uncertainty or randomness in a dataset.\n",
        "It ranges from 0 (certain) to 1 (uncertain).\n",
        "A lower entropy indicates a better split.\n",
        "Formula: Entropy = -Σ pi * log2(pi), where pi is the proportion of instances belonging to class i in a node.\n",
        "Information Gain:\n",
        "\n",
        "Information gain is a measure of the reduction in entropy achieved by splitting the data based on a particular attribute.\n",
        "It is calculated as the difference between the entropy of the parent node and the weighted average entropy of the child nodes.\n",
        "A higher information gain indicates a better split.\n",
        "     \n",
        "     Formula: Information Gain = Entropy(parent) - [Weighted Average] * Entropy(children)\n",
        "\n",
        "#Misclassification Error:\n",
        "\n",
        "Measures the fraction of instances misclassified at a particular node.\n",
        "Range is 0 to 1, where 0 means no error.\n",
        "Formula: Misclassification Error = 1- max(pi), where max(pi) represents the proportion of the majority class\n",
        "How are they used?\n",
        "\n",
        "During tree construction, the decision tree algorithm evaluates the impurity of each potential split using one of these measures.\n",
        "It selects the split that results in the greatest reduction in impurity (or, equivalently, the greatest information gain).\n",
        "This process is repeated recursively until a stopping criterion is met, resulting in a tree that partitions the data into relatively pure subsets.\n",
        "Which measure to use?\n",
        "\n",
        "Gini impurity and entropy are the most commonly used impurity measures in practice.\n",
        "They often produce similar results, and the choice between them is often a matter of personal preference or computational efficiency.\n",
        "Information gain is a more general concept that can be used with any impurity measure, making it a powerful tool for attribute selection in decision trees.\n",
        "Misclassification error, although conceptually simple, might be too sensitive to changes in the dataset and is generally avoided."
      ],
      "metadata": {
        "id": "9NyrmtUomBga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.What is the mathematical formula for Gini impurity?"
      ],
      "metadata": {
        "id": "k3tZOc5RmOpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Gini Impurity = 1 - Σ (pi)^2\n",
        "\n",
        "Where:\n",
        "\n",
        "pi represents the proportion of instances belonging to class 'i' in a particular node.\n",
        "The summation (Σ) is calculated over all classes present in the node.\n",
        "In simpler terms:\n",
        "\n",
        "For each class in the node, calculate the proportion of instances belonging to that class (pi).\n",
        "Square each proportion (pi)^2.\n",
        "Sum up the squared proportions for all classes (Σ (pi)^2).\n",
        "Subtract the sum from 1 to get the Gini impurity.\n",
        "\n",
        "Example:\n",
        "\n",
        "Let's say a node in your decision tree has 10 instances, with 6 belonging to class A and 4 belonging to class B.\n",
        "\n",
        "    Proportion of class A (pA) = 6/10 = 0.6\n",
        "    Proportion of class B (pB) = 4/10 = 0.4\n",
        "    Gini Impurity = 1 - [(pA)^2 + (pB)^2]\n",
        "           = 1 - [(0.6)^2 + (0.4)^2]\n",
        "           = 1 - [0.36 + 0.16]\n",
        "           = 1 - 0.52\n",
        "           = 0.48\n",
        "    Therefore, the Gini impurity for this node is 0.48.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "A Gini impurity of 0 indicates a perfectly pure node, where all instances belong to the same class.\n",
        "A Gini impurity closer to 1 indicates a highly impure node, where instances are evenly distributed among different classes."
      ],
      "metadata": {
        "id": "89W-P5XQmspR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.What is the mathematical formula of entropy ?"
      ],
      "metadata": {
        "id": "Q43rhFHym4pg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Entropy = - Σ pi * log2(pi)\n",
        "\n",
        "Where:\n",
        "\n",
        "pi represents the proportion of instances belonging to class 'i' in a particular node.\n",
        "The summation (Σ) is calculated over all classes present in the node.\n",
        "log2 denotes the logarithm base 2.\n",
        "In simpler terms:\n",
        "\n",
        "For each class in the node, calculate the proportion of instances belonging to that class (pi).\n",
        "Calculate the logarithm base 2 of each proportion (log2(pi)).\n",
        "Multiply each proportion by its logarithm (pi * log2(pi)).\n",
        "Sum up the results for all classes (Σ pi * log2(pi)).\n",
        "Negate the sum to get the entropy.\n",
        "Example:\n",
        "\n",
        "Let's say a node in your decision tree has 10 instances, with 6 belonging to class A and 4 belonging to class B.\n",
        "\n",
        "    Proportion of class A (pA) = 6/10 = 0.6\n",
        "    Proportion of class B (pB) = 4/10 = 0.4\n",
        "    Entropy = - [(pA * log2(pA)) + (pB * log2(pB))]\n",
        "      = - [(0.6 * log2(0.6)) + (0.4 * log2(0.4))]\n",
        "      ≈ - [ (0.6 * -0.737) + (0.4 * -1.322)]\n",
        "      ≈ - [-0.442 + -0.529]\n",
        "      ≈ 0.971\n",
        "    Therefore, the entropy for this node is approximately 0.971.\n",
        "\n",
        "#Interpretation:\n",
        "\n",
        "An entropy of 0 indicates a perfectly pure node, where all instances belong to the same class.\n",
        "An entropy closer to 1 indicates a highly impure node, where instances are evenly distributed among different classes."
      ],
      "metadata": {
        "id": "chvxfRbfnIIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.What is Information Gain,and how does it works in Decision Trees ?"
      ],
      "metadata": {
        "id": "V6iKnKzWnVJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information gain is a concept from information theory, used in decision trees to quantify the effectiveness of a split. Essentially, it measures how much information you gain about the target variable by splitting the data based on a particular attribute. The attribute that yields the highest information gain is chosen as the splitting attribute.\n",
        "\n",
        "#How does it work in Decision Trees?\n",
        "\n",
        "Here's how information gain works in the context of decision trees:\n",
        "\n",
        "Calculate Entropy of the Parent Node: Before the split, calculate the entropy of the target variable in the parent node. This represents the uncertainty or impurity in the data before the split.\n",
        "Calculate Entropy of Child Nodes: For each possible split on an attribute, calculate the entropy of the target variable within each resulting child node.\n",
        "Calculate Weighted Average Entropy of Child Nodes: Calculate the weighted average entropy of the child nodes. The weight for each child node is determined by the proportion of data points that fall into that node.\n",
        "Calculate Information Gain: Finally, subtract the weighted average entropy of the child nodes from the entropy of the parent node. This difference represents the information gain achieved by the split.\n",
        "Formula:\n",
        "\n",
        "       Information Gain = Entropy(parent) - [Weighted Average] * Entropy(children)\n",
        "\n",
        "Example:\n",
        "\n",
        "Let's say you have a dataset with a target variable \"Play Tennis\" (Yes/No) and an attribute \"Weather\" (Sunny/Overcast/Rainy).\n",
        "\n",
        "You calculate the entropy of \"Play Tennis\" in the entire dataset (parent node).\n",
        "You consider splitting the data based on the \"Weather\" attribute. This would create three child nodes: Sunny, Overcast, and Rainy.\n",
        "You calculate the entropy of \"Play Tennis\" within each of these child nodes.\n",
        "You calculate the weighted average entropy of the child nodes, taking into account the proportion of data points in each node.\n",
        "You subtract this weighted average entropy from the parent node's entropy to get the information gain for the \"Weather\" attribute.\n",
        "You repeat this process for other attributes (e.g., Temperature, Humidity) and select the attribute with the highest information gain as the splitting attribute for the current node in the decision tree.\n",
        "\n",
        "In summary:\n",
        "\n",
        "Higher information gain indicates a better split, meaning the split leads to a greater reduction in uncertainty about the target variable.\n",
        "Decision trees use information gain to select the best attribute for splitting the data at each node, aiming to create a tree that accurately classifies or predicts the target variable."
      ],
      "metadata": {
        "id": "o2MF2bvhn1r_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6.What is the difference between Gini Impurity and Entropy?"
      ],
      "metadata": {
        "id": "-Nxa3AQ2n7sW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both Gini Impurity and Entropy are metrics used in decision trees to measure the impurity or uncertainty of a node. They help determine the best attribute to split the data at each node, aiming to create a tree that accurately classifies or predicts the target variable. However, they differ in their calculations and properties:\n",
        "\n",
        "#Gini Impurity:\n",
        "\n",
        "Formula: Gini Impurity = 1 - Σ (pi)^2, where pi is the proportion of instances belonging to class i in a node.\n",
        "Range: 0 (pure) to 1 (impure).\n",
        "\n",
        "Calculation: It calculates the probability of misclassifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.\n",
        "Computation: Generally faster to compute as it involves linear operations.\n",
        "Focus: Emphasizes reducing misclassification.\n",
        "#Entropy:\n",
        "\n",
        "Formula: Entropy = -Σ pi * log2(pi), where pi is the proportion of instances belonging to class i in a node.\n",
        "Range: 0 (certain) to 1 (uncertain).\n",
        "\n",
        "Calculation: It measures the uncertainty or randomness in a dataset.\n",
        "Computation: Can be slightly slower to compute due to the logarithmic calculations.\n",
        "Focus: Measures the disorder or information content in the data.\n",
        "\n",
        "Key Differences:\n",
        "\n",
        "     Feature\t Gini Impurity\t         Entropy\n",
        "    Formula\t   1 - Σ (pi)^2\t        -Σ pi * log2(pi)\n",
        "    Range\t     0 to 1\t                 0 to 1\n",
        "    Calculation\tProbability of misclassification\tUncertainty or randomness\n",
        "    Computation\tFaster\t             Slightly slower\n",
        "    Focus\tReducing misclassification\tInformation content\n",
        "\n",
        "In Practice:\n",
        "\n",
        "Both Gini Impurity and Entropy often produce similar results when building decision trees.\n",
        "The choice between them is often a matter of personal preference or computational considerations.\n",
        "Gini Impurity might be preferred when computational efficiency is a priority.\n",
        "Entropy might be preferred when a more comprehensive measure of information content is desired."
      ],
      "metadata": {
        "id": "5YGAmkvooZ3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.What is the mathematical explanation behind Decision Trees ?"
      ],
      "metadata": {
        "id": "9cT-3hqhpBME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees utilize mathematical concepts to make decisions about splitting data and building the tree structure. Here's a breakdown of the key mathematical aspects:\n",
        "\n",
        "Impurity Measures: These quantify the homogeneity or impurity of a node in the tree. Common impurity measures include:\n",
        "\n",
        "Gini Impurity: Measures the probability of misclassifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.\n",
        "     Formula: Gini Impurity = 1 - Σ (pi)^2\n",
        "\n",
        "    Entropy: Measures the uncertainty or randomness in a dataset. Formula: Entropy = -Σ pi * log2(pi)\n",
        "\n",
        "Information Gain: This concept is used to select the best attribute for splitting the data at each node. It measures the reduction in impurity achieved by splitting the data based on a particular attribute.\n",
        "\n",
        "     Formula: Information Gain = Entropy(parent) - [Weighted Average] * Entropy(children)\n",
        "Recursive Partitioning: Decision trees are built using a recursive partitioning approach. This involves:\n",
        "\n",
        "Starting with the entire dataset as the root node.\n",
        "Selecting the attribute that maximizes information gain.\n",
        "Splitting the data based on the selected attribute's values, creating child nodes.\n",
        "Recursively repeating the process for each child node until a stopping criterion is met (e.g., all instances in a subset belong to the same class, a maximum depth is reached).\n",
        "Tree Pruning: To prevent overfitting, decision trees are often pruned. This involves removing branches that do not significantly improve the model's performance on unseen data. Pruning techniques include:\n",
        "\n",
        "Pre-pruning: Setting limits on tree depth or the minimum number of instances per node during tree construction.\n",
        "Post-pruning: Growing the tree fully and then removing branches based on a validation set or statistical measures.\n",
        "Prediction: Once the tree is built, it can be used to predict the target variable for new instances. This involves traversing the tree based on the instance's attribute values until a leaf node is reached. The prediction is the class label or numerical value associated with the leaf node.\n",
        "\n",
        "In essence, decision trees use these mathematical principles to:\n",
        "\n",
        "Identify the most informative attributes for splitting the data.\n",
        "Partition the data into homogeneous subsets.\n",
        "Create a tree structure that represents the relationships between attributes and the target variable.\n",
        "Make predictions for new instances based on the learned patterns."
      ],
      "metadata": {
        "id": "cS5eAQNaps4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8.What is the Pre-Puning in Decision Trees?"
      ],
      "metadata": {
        "id": "erZV0heMp7Qb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Pre-Pruning\n",
        "\n",
        "Pre-pruning, also known as early stopping, is a technique used in decision tree learning to prevent overfitting by controlling the growth of the tree during the construction process. It involves stopping the tree's growth before it reaches its full potential depth or complexity.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Pre-pruning involves setting constraints or thresholds that limit the tree's growth. These constraints can be based on various factors, such as:\n",
        "\n",
        "Maximum Depth: Limiting the maximum depth of the tree.\n",
        "Minimum Number of Samples per Leaf: Specifying the minimum number of samples required to create a leaf node.\n",
        "Minimum Impurity Decrease: Setting a threshold for the minimum decrease in impurity required for a split to occur.\n",
        "During tree construction, if any of these constraints are violated, the tree's growth is stopped at that point, preventing further splitting.\n",
        "\n",
        "#Benefits of Pre-Pruning:\n",
        "\n",
        "Reduces Overfitting: By limiting the tree's complexity, pre-pruning helps to prevent overfitting, where the tree becomes too specific to the training data and performs poorly on unseen data.\n",
        "Improves Efficiency: Pre-pruning can reduce the computational cost of building the tree by stopping its growth earlier.\n",
        "Simpler Trees: Pre-pruning often results in smaller and simpler trees, which can be easier to interpret and understand.\n",
        "#Limitations of Pre-Pruning:\n",
        "\n",
        "Difficult to Choose Optimal Constraints: Selecting the appropriate constraints for pre-pruning can be challenging and may require experimentation.\n",
        "Potential for Underfitting: If the constraints are too restrictive, pre-pruning can lead to underfitting, where the tree is not complex enough to capture the underlying patterns in the data.\n",
        "In summary, pre-pruning is a valuable technique for controlling the complexity of decision trees and preventing overfitting. By setting appropriate constraints, it helps to create more robust and generalizable models. However, careful consideration is needed to choose the optimal constraints to avoid underfitting."
      ],
      "metadata": {
        "id": "khqzEEkNqQJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9.What is the Post-Pruning In Decision Trees?"
      ],
      "metadata": {
        "id": "5uLw4UzgqfUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Post-Pruning\n",
        "\n",
        "Post-pruning, also known as backward pruning, is a technique used in decision tree learning to reduce overfitting by removing branches from a fully grown tree. It's done after the tree has been constructed, unlike pre-pruning, which stops tree growth early.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Grow the Tree: The decision tree is first allowed to grow to its full depth and complexity, without any restrictions.\n",
        "Prune Branches: Starting from the bottom of the tree, branches are iteratively removed or replaced with leaf nodes if they do not significantly improve the model's performance on a validation dataset or based on statistical measures.\n",
        "Evaluation: The pruning process continues until further pruning starts to decrease the model's performance on the validation set.\n",
        "#Common Post-Pruning Techniques:\n",
        "\n",
        "Reduced Error Pruning: This technique replaces a subtree with a leaf node if it reduces the overall error rate on the validation set.\n",
        "Cost Complexity Pruning: This technique involves assigning a cost to each node based on its complexity and error rate. It then prunes the tree by removing nodes that have a high cost relative to their contribution to reducing error.\n",
        "Minimum Error Pruning: This technique removes subtrees that have a higher error rate than their parent node.\n",
        "#Benefits of Post-Pruning:\n",
        "\n",
        "Effective Overfitting Reduction: Post-pruning often results in more effective overfitting reduction compared to pre-pruning, as it considers the fully grown tree before making pruning decisions.\n",
        "Better Accuracy: By removing unnecessary branches, post-pruning can improve the tree's accuracy on unseen data.\n",
        "Increased Generalization: It helps create more generalizable decision tree models that perform well on a wider range of data.\n",
        "#Limitations of Post-Pruning:\n",
        "\n",
        "Computational Cost: Post-pruning can be computationally expensive, especially for large trees.\n",
        "Complexity: It can be more complex to implement compared to pre-pruning.\n",
        "In summary, post-pruning is a powerful technique for reducing overfitting and improving the generalization performance of decision trees. While it has some computational overhead, it often leads to more accurate and robust models compared to pre-pruning. Choosing between pre-pruning and post-pruning depends on the specific dataset and desired trade-offs between complexity, computational cost, and accuracy."
      ],
      "metadata": {
        "id": "3lzS_htGrw__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10.What is the Difference between Pre-Pruning and Post-Pruning ?"
      ],
      "metadata": {
        "id": "rhrZh4ZLr6ku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees:\n",
        "\n",
        "Both pre-pruning and post-pruning are techniques used to reduce overfitting in decision trees, but they differ in their approach and timing:\n",
        "\n",
        "#Pre-Pruning:\n",
        "\n",
        "Timing: Applied during the tree construction process.\n",
        "Approach: Stops the tree's growth early by setting constraints on tree size or node characteristics.\n",
        "Focus: Prevents the tree from becoming too complex in the first place.\n",
        "Advantages:\n",
        "Reduces overfitting.\n",
        "Improves efficiency by stopping tree growth earlier.\n",
        "Results in simpler, easier-to-interpret trees.\n",
        "\n",
        "Disadvantages:\n",
        "Difficult to choose optimal constraints.\n",
        "Can lead to underfitting if constraints are too restrictive.\n",
        "#Post-Pruning:\n",
        "\n",
        "Timing: Applied after the tree has been fully grown.\n",
        "Approach: Removes branches from the fully grown tree that do not significantly improve performance on a validation set.\n",
        "Focus: Reduces complexity by selectively removing unnecessary parts of the tree.\n",
        "\n",
        "Advantages:\n",
        "More effective overfitting reduction as it considers the fully grown tree.\n",
        "Can improve accuracy on unseen data.\n",
        "Increases generalization performance.\n",
        "Disadvantages:\n",
        "Computationally expensive, especially for large trees.\n",
        "More complex to implement compared to pre-pruning.\n",
        "\n",
        "Key Differences Summarized:\n",
        "\n",
        "    Feature\t  Pre-Pruning\t              Post-Pruning\n",
        "   \n",
        "    Timing\tDuring tree construction\tAfter tree construction\n",
        "\n",
        "    Approach\tStops tree growth early\tRemoves branches from fully grown tree\n",
        "    \n",
        "    Focus\t Prevents complexity\t      Reduces complexity\n",
        "\n",
        "    Overfitting  Reduction\tLess effective\tMore effective\n",
        "    \n",
        "    Computational Cost\tLower\t        Higher\n",
        "\n",
        "    Complexity\tSimpler\t              More complex\n",
        "\n",
        " Choosing between Pre-Pruning and Post-Pruning:\n",
        "\n",
        "The choice between pre-pruning and post-pruning depends on the specific dataset, computational resources, and desired trade-offs between complexity, accuracy, and efficiency.\n",
        "\n",
        "If computational resources are limited or a simpler tree is desired, pre-pruning might be preferred.\n",
        "If maximizing accuracy and generalization performance is the primary goal, post-pruning is often a better choice, despite its higher computational cost.\n",
        "In many cases, a combination of both techniques might be used to achieve the best results."
      ],
      "metadata": {
        "id": "36SirpByslI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11.What is a Decision Tree Regressor?"
      ],
      "metadata": {
        "id": "IZcEIu8Yt0R1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree Regressor is a type of decision tree algorithm used for solving regression problems, where the target variable is continuous rather than categorical. It works by partitioning the data into smaller subsets and fitting a simple model (typically a constant value) to each subset. The predictions are made by traversing the tree based on the input features and arriving at a leaf node, which represents the predicted value.\n",
        "\n",
        "Here's a breakdown of how it works:\n",
        "\n",
        "Tree Construction: Similar to classification trees, the regressor tree is built by recursively splitting the data based on features that best separate the target variable into distinct ranges. However, instead of using impurity measures like Gini impurity or entropy, regression trees use metrics like mean squared error (MSE) or mean absolute error (MAE) to evaluate the quality of a split.\n",
        "\n",
        "Leaf Node Prediction: In a regression tree, the leaf nodes represent a predicted value for the target variable. This value is typically the average or median of the target values of the data points that fall into that leaf node.\n",
        "\n",
        "Prediction for New Data: To predict the target variable for a new data point, the algorithm traverses the tree based on the values of the input features until it reaches a leaf node. The predicted value for the new data point is then the value associated with that leaf node.\n",
        "\n",
        "Example:\n",
        "\n",
        "Imagine you want to predict the price of a house based on features like size, location, and number of bedrooms. A decision tree regressor might create a tree where:\n",
        "\n",
        "The root node splits the data based on the size of the house.\n",
        "Subsequent nodes might split based on location or number of bedrooms.\n",
        "The leaf nodes would represent the predicted price of the house for data points that fall into that specific range of features.\n",
        "\n",
        "#Advantages of Decision Tree Regressors:\n",
        "\n",
        "Easy to understand and interpret.\n",
        "Can handle both numerical and categorical features.\n",
        "Non-parametric, meaning they don't make assumptions about the data distribution.\n",
        "Can capture non-linear relationships between features and the target variable.\n",
        "#Disadvantages of Decision Tree Regressors:\n",
        "\n",
        "Prone to overfitting if not pruned properly.\n",
        "Can be sensitive to small changes in the data.\n",
        "May not perform well on datasets with complex interactions between features.\n",
        "In summary, a Decision Tree Regressor is a powerful algorithm for solving regression problems. It works by partitioning the data into subsets and fitting a simple model to each subset. While it has some limitations, it's often a good choice for its interpretability and ability to handle different types of data."
      ],
      "metadata": {
        "id": "phEOelLuvEJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12.What are the advantages and disadvantages of Decision Trees?"
      ],
      "metadata": {
        "id": "aTDrorICv8o3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Advantages of Decision Trees:\n",
        "\n",
        "Easy to Understand and Interpret: Decision trees are easy to understand and visualize, even for people without a strong technical background. The tree structure clearly shows the decision-making process, making it transparent and interpretable.\n",
        "\n",
        "Handles Both Numerical and Categorical Data: Decision trees can handle both numerical and categorical data without requiring extensive data preprocessing. This flexibility makes them applicable to a wide range of datasets.\n",
        "\n",
        "Non-parametric: Decision trees are non-parametric models, meaning they don't make assumptions about the underlying data distribution. This makes them robust to outliers and data with non-linear relationships.\n",
        "\n",
        "Feature Selection: Decision trees can be used for feature selection by identifying the most important attributes for splitting the data. This can help in understanding the data and reducing dimensionality.\n",
        "\n",
        "Requires Little Data Preparation: Compared to some other algorithms, decision trees require relatively little data preparation. They can handle missing values and don't require feature scaling.\n",
        "\n",
        "#Disadvantages of Decision Trees:\n",
        "\n",
        "Prone to Overfitting: Decision trees can easily overfit the training data, especially when they are deep and complex. This can lead to poor generalization performance on unseen data. Techniques like pruning and using ensemble methods can help mitigate overfitting.\n",
        "\n",
        "Instability: Decision trees can be unstable, meaning small changes in the data can lead to significant changes in the tree structure. This can make them less reliable in certain applications.\n",
        "\n",
        "Limited on Complex Relationships: Decision trees may not perform well on datasets with complex interactions between features. They tend to create rectangular decision boundaries, which may not be suitable for capturing intricate patterns.\n",
        "\n",
        "Bias Towards Dominant Classes: In classification problems with imbalanced datasets, decision trees can be biased towards the dominant classes. Techniques like using weighted samples or adjusting the splitting criteria can help address this issue.\n",
        "\n",
        "Computational Cost: Building and pruning large decision trees can be computationally expensive, especially for datasets with many features and instances."
      ],
      "metadata": {
        "id": "uYUroIiiwjzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13.How does a Decision Tree handle missing values?"
      ],
      "metadata": {
        "id": "alY3U78zwoTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees have inherent mechanisms to handle missing values during both the tree construction and prediction phases. Here's a breakdown of the common approaches:\n",
        "\n",
        "#During Tree Construction:\n",
        "\n",
        "Ignoring Missing Values: The simplest approach is to ignore instances with missing values for the specific attribute being considered for a split. This can lead to a loss of information, but it's computationally efficient.\n",
        "\n",
        "Imputation: Missing values can be imputed (filled in) using various techniques, such as replacing them with the mean, median, or mode of the attribute. This allows all instances to be used for splitting, but it can introduce bias if the imputation is inaccurate.\n",
        "\n",
        "Surrogate Splits: This approach involves finding a surrogate attribute that is highly correlated with the attribute containing missing values. When an instance has a missing value for the primary attribute, the surrogate attribute is used to determine the split instead. This helps preserve information and avoid bias. This is the most common way decision trees handle missing data.\n",
        "\n",
        "#During Prediction:\n",
        "\n",
        "Following the Most Frequent Branch: If an instance encounters a missing value for an attribute during prediction, it can be directed down the branch that was most frequently taken by training instances with similar values for other attributes. This is a heuristic approach that may not always be accurate.\n",
        "\n",
        "Probabilistic Approach: A more sophisticated approach is to assign probabilities to each possible branch based on the distribution of training instances with missing values. The instance is then directed down each branch with the corresponding probability, and the final prediction is a weighted average of the predictions from all branches. This approach provides a more nuanced prediction that accounts for uncertainty.\n",
        "\n",
        "In summary, decision trees offer various ways to handle missing values, primarily using surrogate splits during training. The choice of approach depends on the specific dataset, the amount of missing data, and the desired trade-off between accuracy and computational efficiency.\n",
        "\n",
        "#Here are some additional points to consider:\n",
        "\n",
        "Decision tree algorithms like CART (Classification and Regression Trees) and C4.5 have built-in mechanisms for handling missing values, often using surrogate splits or probabilistic approaches.\n",
        "The effectiveness of different missing value handling techniques can vary depending on the dataset and the specific problem.\n",
        "It's generally recommended to experiment with different approaches and evaluate their performance on a validation set to choose the best strategy for your specific use case."
      ],
      "metadata": {
        "id": "q-heXff-xSQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14.How does a Decision Tree handle Categorical features ?"
      ],
      "metadata": {
        "id": "2-E71SvIxavc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees can handle categorical features without requiring one-hot encoding. Here's how they do it:\n",
        "\n",
        "#During Tree Construction:\n",
        "\n",
        "Splitting on Categorical Features: When considering a categorical feature for a split, the decision tree algorithm evaluates all possible subsets of categories. For example, if a feature has categories {Red, Green, Blue}, the algorithm would consider splits like {Red} vs. {Green, Blue}, {Green} vs. {Red, Blue}, and {Blue} vs. {Red, Green}.\n",
        "\n",
        "Evaluating Splits: The algorithm uses impurity measures (e.g., Gini impurity, entropy) or information gain to evaluate the quality of each split. The split that results in the greatest reduction in impurity or the highest information gain is selected.\n",
        "\n",
        "Creating Branches: For the selected split, branches are created for each subset of categories. Each branch represents a specific combination of categories for the feature.\n",
        "#During Prediction:\n",
        "\n",
        "Traversing the Tree: When predicting for a new instance, the algorithm traverses the tree based on the values of the categorical features. At each node, it checks which branch corresponds to the instance's category for the feature being considered.\n",
        "\n",
        "Reaching a Leaf Node: The algorithm continues traversing the tree until it reaches a leaf node, which provides the prediction for the instance.\n",
        "\n",
        "Example:\n",
        "\n",
        "Let's say you have a dataset with a categorical feature \"Color\" (Red, Green, Blue) and a target variable \"Fruit\" (Apple, Orange).\n",
        "\n",
        "During tree construction, the algorithm might find that splitting on \"Color\" with the subsets {Red} vs. {Green, Blue} results in the highest information gain.\n",
        "\n",
        "This split would create two branches: one for instances with \"Color\" equal to \"Red\" and another for instances with \"Color\" equal to \"Green\" or \"Blue\".\n",
        "\n",
        "During prediction, if a new instance has \"Color\" equal to \"Red\", it would follow the corresponding branch and potentially reach a leaf node predicting \"Apple\".\n",
        "\n",
        "In summary, decision trees can effectively handle categorical features by considering all possible subsets of categories for splits and creating branches accordingly. This allows them to capture the relationships between categorical features and the target variable without the need for one-hot encoding.\n",
        "\n",
        "Here's an additional point to consider:\n",
        "\n",
        "Some decision tree implementations, like scikit-learn's DecisionTreeClassifier, might have limitations in directly handling categorical features with high cardinality (many unique categories). In such cases, it's often recommended to use techniques like ordinal encoding or target encoding to represent the categorical features."
      ],
      "metadata": {
        "id": "sAB1nL71x7EV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#15.What are same real-world applications of Decision Trees?"
      ],
      "metadata": {
        "id": "Jv5goV9oyGYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees are versatile machine learning models used across various domains for their interpretability and ability to handle both numerical and categorical data. Here are some notable real-world applications:\n",
        "\n",
        "1. Customer Churn Prediction: Businesses use decision trees to predict which customers are likely to churn (cancel their subscriptions or services). By analyzing customer demographics, usage patterns, and interactions, decision trees can identify at-risk customers, enabling proactive retention strategies.\n",
        "\n",
        "2. Fraud Detection: Decision trees are employed in fraud detection systems to identify suspicious transactions. By learning patterns from historical data on fraudulent activities, decision trees can flag potentially fraudulent transactions for further investigation.\n",
        "\n",
        "3. Medical Diagnosis: Decision trees assist medical professionals in diagnosing diseases by analyzing patient symptoms, medical history, and test results. They can help identify potential conditions and recommend further tests or treatments.\n",
        "\n",
        "4. Credit Risk Assessment: Financial institutions utilize decision trees to assess the creditworthiness of loan applicants. By considering factors like income, credit history, and debt levels, decision trees can predict the likelihood of loan defaults, aiding in loan approval decisions.\n",
        "\n",
        "5. Marketing Campaign Optimization: Decision trees can segment customers based on their preferences and behaviors, allowing marketers to target specific groups with personalized campaigns. This can improve campaign effectiveness and return on investment.\n",
        "\n",
        "6. Image Classification: Decision trees are used in image classification tasks, such as identifying objects or scenes in images. They can learn patterns from image features to categorize images into different classes.\n",
        "\n",
        "7. Natural Language Processing: Decision trees can be applied in natural language processing tasks like sentiment analysis and text classification. They can analyze text data to determine the sentiment expressed or categorize text into different topics.\n",
        "\n",
        "These are just a few examples of the many real-world applications of decision trees. Their versatility and interpretability make them valuable tools for solving a wide range of problems across diverse industries. I hope this gives you a better understanding of their practical uses!"
      ],
      "metadata": {
        "id": "Tro1OJdHyqpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical"
      ],
      "metadata": {
        "id": "43Qhxka3y0yj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#16.Write a python program to train a Decision Tree Classifier on the iris dataset and print the model accuracy?"
      ],
      "metadata": {
        "id": "YYYUipYRy4kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier object\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train the model using the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWCD20Dgziti",
        "outputId": "df84bf07-36b3-4937-e6a5-214643450032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#17.Write a python program to train Decision Tree Classifier using Gini impurity as the criterion and print the feature importances."
      ],
      "metadata": {
        "id": "Q1fnmuTjzoa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier object with Gini impurity as the criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "\n",
        "# Train the model using the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipCik46D0VjQ",
        "outputId": "97d22396-4eae-4f6b-b6fd-9526ef9ade35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.01911001911001911\n",
            "sepal width (cm): 0.01911001911001911\n",
            "petal length (cm): 0.5381637432635216\n",
            "petal width (cm): 0.42361621851644027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#18.Write a python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy."
      ],
      "metadata": {
        "id": "yO4OQg5W0Xux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier object with Entropy as the criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"entropy\")\n",
        "\n",
        "# Train the model using the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L30nN6rK1UIe",
        "outputId": "a91127d1-3c9b-4fec-baab-b86774fa6b49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#19.Write a python program to train a Decision Tree Regressor on housing dataset and evaluate using Mean Squared Error (MSE) ."
      ],
      "metadata": {
        "id": "V5fSIwmU1YJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor object\n",
        "regressor = DecisionTreeRegressor()\n",
        "\n",
        "# Train the model using the training data\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print the MSE\n",
        "print(\"Mean Squared Error (MSE):\", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEXdE5oF2i0N",
        "outputId": "f030821a-6d57-49d7-946c-aab6db2a09b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.5000749163024951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#20.Write a python program to train a Decision Tree Classifier and Visualize the tree using graphviz."
      ],
      "metadata": {
        "id": "ChwzSbkf20tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier object\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train the model using the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Export the decision tree to a DOT file\n",
        "dot_data = export_graphviz(clf, out_file=None,\n",
        "                         feature_names=iris.feature_names,\n",
        "                         class_names=iris.target_names,\n",
        "                         filled=True, rounded=True,\n",
        "                         special_characters=True)\n",
        "\n",
        "# Create a graphviz object from the DOT data\n",
        "graph = graphviz.Source(dot_data)\n",
        "\n",
        "# Render the graph to a file or display it inline\n",
        "graph.render(\"iris_decision_tree\")  # Save as PDF\n",
        "graph # Display in Colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "4Y-ZBGBW3jFD",
        "outputId": "2c9ebee0-5050-472a-f573-10010cbd678b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"852pt\" height=\"671pt\"\n viewBox=\"0.00 0.00 852.00 671.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 667)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-667 848,-667 848,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M506.5,-663C506.5,-663 384.5,-663 384.5,-663 378.5,-663 372.5,-657 372.5,-651 372.5,-651 372.5,-592 372.5,-592 372.5,-586 378.5,-580 384.5,-580 384.5,-580 506.5,-580 506.5,-580 512.5,-580 518.5,-586 518.5,-592 518.5,-592 518.5,-651 518.5,-651 518.5,-657 512.5,-663 506.5,-663\"/>\n<text text-anchor=\"start\" x=\"380.5\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 0.8</text>\n<text text-anchor=\"start\" x=\"410\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.664</text>\n<text text-anchor=\"start\" x=\"400.5\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 105</text>\n<text text-anchor=\"start\" x=\"387.5\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 37, 37]</text>\n<text text-anchor=\"start\" x=\"393\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M415,-536.5C415,-536.5 322,-536.5 322,-536.5 316,-536.5 310,-530.5 310,-524.5 310,-524.5 310,-480.5 310,-480.5 310,-474.5 316,-468.5 322,-468.5 322,-468.5 415,-468.5 415,-468.5 421,-468.5 427,-474.5 427,-480.5 427,-480.5 427,-524.5 427,-524.5 427,-530.5 421,-536.5 415,-536.5\"/>\n<text text-anchor=\"start\" x=\"340.5\" y=\"-521.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"327.5\" y=\"-506.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 31</text>\n<text text-anchor=\"start\" x=\"318\" y=\"-491.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 0, 0]</text>\n<text text-anchor=\"start\" x=\"325\" y=\"-476.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M418.79,-579.91C411.38,-568.65 403.33,-556.42 395.88,-545.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"398.75,-543.1 390.33,-536.67 392.9,-546.94 398.75,-543.1\"/>\n<text text-anchor=\"middle\" x=\"385.28\" y=\"-557.45\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M587.5,-544C587.5,-544 457.5,-544 457.5,-544 451.5,-544 445.5,-538 445.5,-532 445.5,-532 445.5,-473 445.5,-473 445.5,-467 451.5,-461 457.5,-461 457.5,-461 587.5,-461 587.5,-461 593.5,-461 599.5,-467 599.5,-473 599.5,-473 599.5,-532 599.5,-532 599.5,-538 593.5,-544 587.5,-544\"/>\n<text text-anchor=\"start\" x=\"453.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.75</text>\n<text text-anchor=\"start\" x=\"494.5\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"481.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 74</text>\n<text text-anchor=\"start\" x=\"468\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 37, 37]</text>\n<text text-anchor=\"start\" x=\"470\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M472.21,-579.91C478.01,-571.1 484.2,-561.7 490.18,-552.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"493.26,-554.3 495.83,-544.02 487.41,-550.45 493.26,-554.3\"/>\n<text text-anchor=\"middle\" x=\"500.88\" y=\"-564.81\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#54e992\" stroke=\"black\" d=\"M478,-425C478,-425 343,-425 343,-425 337,-425 331,-419 331,-413 331,-413 331,-354 331,-354 331,-348 337,-342 343,-342 343,-342 478,-342 478,-342 484,-342 490,-348 490,-354 490,-354 490,-413 490,-413 490,-419 484,-425 478,-425\"/>\n<text text-anchor=\"start\" x=\"339\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.95</text>\n<text text-anchor=\"start\" x=\"375\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.214</text>\n<text text-anchor=\"start\" x=\"369.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 41</text>\n<text text-anchor=\"start\" x=\"360\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 36, 5]</text>\n<text text-anchor=\"start\" x=\"358\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M483.64,-460.91C474.87,-451.74 465.47,-441.93 456.44,-432.49\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"458.73,-429.82 449.29,-425.02 453.68,-434.66 458.73,-429.82\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<path fill=\"#853fe6\" stroke=\"black\" d=\"M702,-425C702,-425 567,-425 567,-425 561,-425 555,-419 555,-413 555,-413 555,-354 555,-354 555,-348 561,-342 567,-342 567,-342 702,-342 702,-342 708,-342 714,-348 714,-354 714,-354 714,-413 714,-413 714,-419 708,-425 702,-425\"/>\n<text text-anchor=\"start\" x=\"563\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.85</text>\n<text text-anchor=\"start\" x=\"599\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.059</text>\n<text text-anchor=\"start\" x=\"593.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 33</text>\n<text text-anchor=\"start\" x=\"584\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 32]</text>\n<text text-anchor=\"start\" x=\"586\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 2&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>2&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M561.36,-460.91C570.13,-451.74 579.53,-441.93 588.56,-432.49\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"591.32,-434.66 595.71,-425.02 586.27,-429.82 591.32,-434.66\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#3fe685\" stroke=\"black\" d=\"M256.5,-306C256.5,-306 134.5,-306 134.5,-306 128.5,-306 122.5,-300 122.5,-294 122.5,-294 122.5,-235 122.5,-235 122.5,-229 128.5,-223 134.5,-223 134.5,-223 256.5,-223 256.5,-223 262.5,-223 268.5,-229 268.5,-235 268.5,-235 268.5,-294 268.5,-294 268.5,-300 262.5,-306 256.5,-306\"/>\n<text text-anchor=\"start\" x=\"130.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.6</text>\n<text text-anchor=\"start\" x=\"160\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.056</text>\n<text text-anchor=\"start\" x=\"154.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 35</text>\n<text text-anchor=\"start\" x=\"145\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 34, 1]</text>\n<text text-anchor=\"start\" x=\"143\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M335.91,-341.91C317.06,-331.65 296.72,-320.58 277.53,-310.14\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"278.97,-306.94 268.51,-305.23 275.62,-313.09 278.97,-306.94\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M475.5,-306C475.5,-306 345.5,-306 345.5,-306 339.5,-306 333.5,-300 333.5,-294 333.5,-294 333.5,-235 333.5,-235 333.5,-229 339.5,-223 345.5,-223 345.5,-223 475.5,-223 475.5,-223 481.5,-223 487.5,-229 487.5,-235 487.5,-235 487.5,-294 487.5,-294 487.5,-300 481.5,-306 475.5,-306\"/>\n<text text-anchor=\"start\" x=\"341.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.55</text>\n<text text-anchor=\"start\" x=\"375\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"373\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n<text text-anchor=\"start\" x=\"363.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 4]</text>\n<text text-anchor=\"start\" x=\"362\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 3&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>3&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M410.5,-341.91C410.5,-333.65 410.5,-324.86 410.5,-316.3\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"414,-316.02 410.5,-306.02 407,-316.02 414,-316.02\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M109,-179.5C109,-179.5 12,-179.5 12,-179.5 6,-179.5 0,-173.5 0,-167.5 0,-167.5 0,-123.5 0,-123.5 0,-117.5 6,-111.5 12,-111.5 12,-111.5 109,-111.5 109,-111.5 115,-111.5 121,-117.5 121,-123.5 121,-123.5 121,-167.5 121,-167.5 121,-173.5 115,-179.5 109,-179.5\"/>\n<text text-anchor=\"start\" x=\"32.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"19.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 34</text>\n<text text-anchor=\"start\" x=\"10\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 34, 0]</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 4&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>4&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M148.66,-222.91C135.04,-211.1 120.17,-198.22 106.6,-186.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"108.62,-183.57 98.77,-179.67 104.03,-188.86 108.62,-183.57\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M240,-179.5C240,-179.5 151,-179.5 151,-179.5 145,-179.5 139,-173.5 139,-167.5 139,-167.5 139,-123.5 139,-123.5 139,-117.5 145,-111.5 151,-111.5 151,-111.5 240,-111.5 240,-111.5 246,-111.5 252,-117.5 252,-123.5 252,-123.5 252,-167.5 252,-167.5 252,-173.5 246,-179.5 240,-179.5\"/>\n<text text-anchor=\"start\" x=\"167.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"158\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"148.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"147\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 4&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>4&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M195.5,-222.91C195.5,-212.2 195.5,-200.62 195.5,-189.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"199,-189.67 195.5,-179.67 192,-189.67 199,-189.67\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M371,-179.5C371,-179.5 282,-179.5 282,-179.5 276,-179.5 270,-173.5 270,-167.5 270,-167.5 270,-123.5 270,-123.5 270,-117.5 276,-111.5 282,-111.5 282,-111.5 371,-111.5 371,-111.5 377,-111.5 383,-117.5 383,-123.5 383,-123.5 383,-167.5 383,-167.5 383,-173.5 377,-179.5 371,-179.5\"/>\n<text text-anchor=\"start\" x=\"298.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"289\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"279.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 3]</text>\n<text text-anchor=\"start\" x=\"278\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 7&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>7&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M381.36,-222.91C373.28,-211.65 364.49,-199.42 356.37,-188.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"358.99,-185.75 350.31,-179.67 353.3,-189.83 358.99,-185.75\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"#9cf2c0\" stroke=\"black\" d=\"M548,-187C548,-187 413,-187 413,-187 407,-187 401,-181 401,-175 401,-175 401,-116 401,-116 401,-110 407,-104 413,-104 413,-104 548,-104 548,-104 554,-104 560,-110 560,-116 560,-116 560,-175 560,-175 560,-181 554,-187 548,-187\"/>\n<text text-anchor=\"start\" x=\"409\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 5.45</text>\n<text text-anchor=\"start\" x=\"445\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"443\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"433.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 1]</text>\n<text text-anchor=\"start\" x=\"428\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 7&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>7&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M434.79,-222.91C440.05,-214.1 445.68,-204.7 451.12,-195.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"454.13,-197.4 456.26,-187.02 448.12,-193.81 454.13,-197.4\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M461,-68C461,-68 364,-68 364,-68 358,-68 352,-62 352,-56 352,-56 352,-12 352,-12 352,-6 358,0 364,0 364,0 461,0 461,0 467,0 473,-6 473,-12 473,-12 473,-56 473,-56 473,-62 467,-68 461,-68\"/>\n<text text-anchor=\"start\" x=\"384.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"375\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"365.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"360\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 9&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>9&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M455.18,-103.73C449.74,-94.97 443.99,-85.7 438.52,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"441.43,-74.95 433.18,-68.3 435.48,-78.64 441.43,-74.95\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M592,-68C592,-68 503,-68 503,-68 497,-68 491,-62 491,-56 491,-56 491,-12 491,-12 491,-6 497,0 503,0 503,0 592,0 592,0 598,0 604,-6 604,-12 604,-12 604,-56 604,-56 604,-62 598,-68 592,-68\"/>\n<text text-anchor=\"start\" x=\"519.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"510\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"500.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"499\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 9&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M505.45,-103.73C510.81,-94.97 516.48,-85.7 521.86,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"524.89,-78.66 527.12,-68.3 518.92,-75 524.89,-78.66\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M697,-306C697,-306 572,-306 572,-306 566,-306 560,-300 560,-294 560,-294 560,-235 560,-235 560,-229 566,-223 572,-223 572,-223 697,-223 697,-223 703,-223 709,-229 709,-235 709,-235 709,-294 709,-294 709,-300 703,-306 697,-306\"/>\n<text text-anchor=\"start\" x=\"568\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sepal width (cm) ≤ 3.1</text>\n<text text-anchor=\"start\" x=\"599\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"597\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"587.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 2]</text>\n<text text-anchor=\"start\" x=\"586\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 12&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>12&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"black\" d=\"M634.5,-341.91C634.5,-333.65 634.5,-324.86 634.5,-316.3\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"638,-316.02 634.5,-306.02 631,-316.02 638,-316.02\"/>\n</g>\n<!-- 16 -->\n<g id=\"node17\" class=\"node\">\n<title>16</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M832,-298.5C832,-298.5 739,-298.5 739,-298.5 733,-298.5 727,-292.5 727,-286.5 727,-286.5 727,-242.5 727,-242.5 727,-236.5 733,-230.5 739,-230.5 739,-230.5 832,-230.5 832,-230.5 838,-230.5 844,-236.5 844,-242.5 844,-242.5 844,-286.5 844,-286.5 844,-292.5 838,-298.5 832,-298.5\"/>\n<text text-anchor=\"start\" x=\"757.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"744.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 30</text>\n<text text-anchor=\"start\" x=\"735\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 30]</text>\n<text text-anchor=\"start\" x=\"737\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 12&#45;&gt;16 -->\n<g id=\"edge16\" class=\"edge\">\n<title>12&#45;&gt;16</title>\n<path fill=\"none\" stroke=\"black\" d=\"M686.89,-341.91C702.27,-329.99 719.07,-316.98 734.37,-305.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"736.93,-307.56 742.69,-298.67 732.65,-302.03 736.93,-307.56\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M679,-179.5C679,-179.5 590,-179.5 590,-179.5 584,-179.5 578,-173.5 578,-167.5 578,-167.5 578,-123.5 578,-123.5 578,-117.5 584,-111.5 590,-111.5 590,-111.5 679,-111.5 679,-111.5 685,-111.5 691,-117.5 691,-123.5 691,-123.5 691,-167.5 691,-167.5 691,-173.5 685,-179.5 679,-179.5\"/>\n<text text-anchor=\"start\" x=\"606.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"597\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"587.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 2]</text>\n<text text-anchor=\"start\" x=\"586\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 13&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>13&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"black\" d=\"M634.5,-222.91C634.5,-212.2 634.5,-200.62 634.5,-189.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"638,-189.67 634.5,-179.67 631,-189.67 638,-189.67\"/>\n</g>\n<!-- 15 -->\n<g id=\"node16\" class=\"node\">\n<title>15</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M818,-179.5C818,-179.5 721,-179.5 721,-179.5 715,-179.5 709,-173.5 709,-167.5 709,-167.5 709,-123.5 709,-123.5 709,-117.5 715,-111.5 721,-111.5 721,-111.5 818,-111.5 818,-111.5 824,-111.5 830,-117.5 830,-123.5 830,-123.5 830,-167.5 830,-167.5 830,-173.5 824,-179.5 818,-179.5\"/>\n<text text-anchor=\"start\" x=\"741.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"732\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"722.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 0]</text>\n<text text-anchor=\"start\" x=\"717\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 13&#45;&gt;15 -->\n<g id=\"edge15\" class=\"edge\">\n<title>13&#45;&gt;15</title>\n<path fill=\"none\" stroke=\"black\" d=\"M681.34,-222.91C694.96,-211.1 709.83,-198.22 723.4,-186.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"725.97,-188.86 731.23,-179.67 721.38,-183.57 725.97,-188.86\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x7a9b82953490>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#21.Write a python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree."
      ],
      "metadata": {
        "id": "U_Pw1GnZ3mXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier with maximum depth of 3\n",
        "clf_limited_depth = DecisionTreeClassifier(max_depth=3)\n",
        "\n",
        "# Create a fully grown Decision Tree Classifier\n",
        "clf_full_depth = DecisionTreeClassifier()\n",
        "\n",
        "# Train the models\n",
        "clf_limited_depth.fit(X_train, y_train)\n",
        "clf_full_depth.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_limited_depth = clf_limited_depth.predict(X_test)\n",
        "y_pred_full_depth = clf_full_depth.predict(X_test)\n",
        "\n",
        "# Calculate accuracy scores\n",
        "accuracy_limited_depth = accuracy_score(y_test, y_pred_limited_depth)\n",
        "accuracy_full_depth = accuracy_score(y_test, y_pred_full_depth)\n",
        "\n",
        "# Print the accuracy scores\n",
        "print(\"Accuracy (Limited Depth):\", accuracy_limited_depth)\n",
        "print(\"Accuracy (Full Depth):\", accuracy_full_depth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo9gB0g54XMQ",
        "outputId": "e8d6ace4-9110-48f4-ff66-77f712479c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Limited Depth): 1.0\n",
            "Accuracy (Full Depth): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#23.Write a python Program to apply feature scaling before training a Decision Tree Classifier and Compare its accuracy with unscaled data."
      ],
      "metadata": {
        "id": "DuYcMyeV4ZI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a StandardScaler object for feature scaling\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply feature scaling to the training data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Apply the same scaling to the testing data\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train a Decision Tree Classifier with scaled data\n",
        "clf_scaled = DecisionTreeClassifier()\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Create and train a Decision Tree Classifier with unscaled data\n",
        "clf_unscaled = DecisionTreeClassifier()\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "\n",
        "# Calculate accuracy scores\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Print the accuracy scores\n",
        "print(\"Accuracy (Scaled Data):\", accuracy_scaled)\n",
        "print(\"Accuracy (Unscaled Data):\", accuracy_unscaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XztS14eG4-JB",
        "outputId": "84ab421f-80db-4fac-ff65-5a33afde2d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Scaled Data): 1.0\n",
            "Accuracy (Unscaled Data): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#24.Write a python Program to train a Decision Tree Classifier using One-vs-Rest(OvR) Stretegy for multiclass classification."
      ],
      "metadata": {
        "id": "JKSIKltl5B_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier as the base estimator\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "\n",
        "# Create a OneVsRestClassifier using the Decision Tree Classifier\n",
        "ovr_classifier = OneVsRestClassifier(estimator=base_estimator)\n",
        "\n",
        "# Train the OneVsRestClassifier\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = ovr_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy (One-vs-Rest):\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVuZR5sM5_gn",
        "outputId": "a52b14c6-9c28-4bd1-ed41-dc6e713ad510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (One-vs-Rest): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#25.Write a python program to train a Decision Tree Classifier and display the feature importance scores."
      ],
      "metadata": {
        "id": "l596AF5g6BeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "importance_scores = clf.feature_importances_\n",
        "\n",
        "# Display feature importance scores along with feature names\n",
        "for feature_name, importance_score in zip(iris.feature_names, importance_scores):\n",
        "    print(f\"{feature_name}: {importance_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhGFdfT46sl2",
        "outputId": "fd9ff0ed-98f2-4530-a877-b46e5d438b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sepal length (cm): 0.0\n",
            "sepal width (cm): 0.01911001911001911\n",
            "petal length (cm): 0.8932635518001373\n",
            "petal width (cm): 0.08762642908984374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#26.Write a python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree."
      ],
      "metadata": {
        "id": "6kalnfCZ6zkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "california = fetch_california_housing()\n",
        "X, y = california.data, california.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Decision Tree Regressor with max_depth=5\n",
        "regressor_depth5 = DecisionTreeRegressor(max_depth=5)\n",
        "regressor_depth5.fit(X_train, y_train)\n",
        "\n",
        "# Create and train an unrestricted Decision Tree Regressor\n",
        "regressor_unrestricted = DecisionTreeRegressor()\n",
        "regressor_unrestricted.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred_depth5 = regressor_depth5.predict(X_test)\n",
        "y_pred_unrestricted = regressor_unrestricted.predict(X_test)\n",
        "\n",
        "# Evaluate the models using Mean Squared Error (MSE)\n",
        "mse_depth5 = mean_squared_error(y_test, y_pred_depth5)\n",
        "mse_unrestricted = mean_squared_error(y_test, y_pred_unrestricted)\n",
        "\n",
        "# Print the results\n",
        "print(\"Decision Tree with max_depth=5:\")\n",
        "print(\"Mean Squared Error (MSE):\", mse_depth5)\n",
        "print(\"\\nUnrestricted Decision Tree:\")\n",
        "print(\"Mean Squared Error (MSE):\", mse_unrestricted)\n",
        "\n",
        "# Compare the results\n",
        "if mse_depth5 < mse_unrestricted:\n",
        "    print(\"\\nDecision Tree with max_depth=5 performs better.\")\n",
        "else:\n",
        "    print(\"\\nUnrestricted Decision Tree performs better.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcQpBP9A72pi",
        "outputId": "b4d7e2df-a85c-472b-f8d0-14995f2103ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with max_depth=5:\n",
            "Mean Squared Error (MSE): 0.5245146178314736\n",
            "\n",
            "Unrestricted Decision Tree:\n",
            "Mean Squared Error (MSE): 0.5017159621065406\n",
            "\n",
            "Unrestricted Decision Tree performs better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#27.Write a python Program to train a Decision Tree Classifier,apply cost complexity pruning (CCP),and visualize its effect on accuracy."
      ],
      "metadata": {
        "id": "d_23paKX8fjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with no pruning\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy_no_pruning = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy without pruning: {accuracy_no_pruning:.4f}\")\n",
        "\n",
        "# Apply cost complexity pruning (CCP)\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "# Train and evaluate models with different CCP alpha values\n",
        "accuracies = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
        "    clf_pruned.fit(X_train, y_train)\n",
        "    y_pred_pruned = clf_pruned.predict(X_test)\n",
        "    accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "    accuracies.append(accuracy_pruned)\n",
        "\n",
        "# Visualize the effect of CCP alpha on accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(ccp_alphas, accuracies, marker='o', drawstyle=\"steps-post\")\n",
        "plt.xlabel(\"ccp_alpha\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy vs. ccp_alpha for Decision Tree\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "QGje7Y7o9cJu",
        "outputId": "f1ee95c6-44fa-4d52-c187-97356d153b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without pruning: 0.9474\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#28.Write a python program to train a Decision Tree Classifier and evaluate its percformance using Precision Recall,and F1-score."
      ],
      "metadata": {
        "id": "BtIjBVvNDwix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree Classifier\n",
        "classifier = DecisionTreeClassifier()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrhuWbflFSfv",
        "outputId": "c163dd4e-1a0a-4c1f-9745-87a435024085"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9385964912280702\n",
            "Precision: 0.9444444444444444\n",
            "Recall: 0.9577464788732394\n",
            "F1-score: 0.951048951048951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#29.Write a python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn."
      ],
      "metadata": {
        "id": "qvAf3DLlFWnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# Load the breast cancer dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test_size and random_state as needed\n",
        "\n",
        "# Create and train the Decision Tree Classifier\n",
        "classifier = DecisionTreeClassifier()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')  # 'd' for integers, 'Blues' for colormap\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()  # Display the plot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "y76YyX-LF2-m",
        "outputId": "60a0c1fa-60c9-43d6-f196-a43f2cd07c8d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#30.Write a python program to train a Decision Tree Classifier and use GridsearchCV to find the optimal values for max_depth and min_samples_split."
      ],
      "metadata": {
        "id": "rYRSNVp0F5aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],  # Explore different values for max_depth\n",
        "    'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10]  # Explore different values for min_samples_split\n",
        "}\n",
        "\n",
        "# Create a Decision Tree Classifier object\n",
        "classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(classifier, param_grid, cv=5, scoring='accuracy')  # cv=5 for 5-fold cross-validation\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_max_depth = grid_search.best_params_['max_depth']\n",
        "best_min_samples_split = grid_search.best_params_['min_samples_split']\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"Best max_depth: {best_max_depth}\")\n",
        "print(f\"Best min_samples_split: {best_min_samples_split}\")\n",
        "\n",
        "# Train a new Decision Tree Classifier with the best hyperparameters\n",
        "best_classifier = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_split=best_min_samples_split)\n",
        "best_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy with best hyperparameters: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izEWHqlPGuHU",
        "outputId": "b8395013-3377-43e9-e8fa-33f621d3bcf5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best max_depth: 5\n",
            "Best min_samples_split: 9\n",
            "Accuracy with best hyperparameters: 0.9385964912280702\n"
          ]
        }
      ]
    }
  ]
}